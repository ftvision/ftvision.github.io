---
title: Introduction
subtitle: 
layout: single
---

Vision is intelligent. It provides sophisticated solutions to difficult problems. What is difficult in vision? You may wonder. It seems so easy and effortless. When we open our eyes, we perceive the world and understand what we see immediately. We only start to appreciate our visual intelligence when we try to work on the problems that our visual system encounters: to infer the world based on images on our retina. The world is distal, three-dimensional. The retinal images are proximal, two-dimensional. Retinal images provide all the information that our visual system processes, but only contains incomplete and insufficient information of the world: the same retinal image may correspond to an infinite number of possible distal stimuli in the world, as illustrated in Figure 1. Among these infinitely many possibilities, we have only one single and stable conscious perception of the world at each time. How does our visual system make its decisions among all choices and give rise to a meaningful conscious perception?
 
![Figure 1](/assets/img/thesis-figure/figure1.png) 
Figure 1: the same retinal image may correspond to an infinite number of possible distal stimuli

Our visual system uses a set of computations to search for the best inference of all possibilities. Among all inference problems in vision, this review focuses on a computational theory of how vision infers locations of distal stimuli. For a detailed computational theory, we need to understand two things: representations that describe information and algorithms that process it (Marr, 1982). Information, however, can be described in different formats and can be processed by different algorithms. What is our visual system’s choice of representations and algorithms? Our work is to reverse engineer what has been designed in our visual system, and understand the reasons and effects of such design. This review, in particular, intends to fill in a theoretical gap in visual location inferences: what is the format of visual location representations. It is a surprising gap because we already know that the visual neural system has a topological organization that neurons near each other tend to respond to information on proximate retinal regions. Such a topological organization, called retinotopy, widely exists in different brain areas (Silver and Kastner, 2009; Wandell and Winawer, 2011). It seems to spatially coordinate information from different areas in visual computations, such as in visual attention (Saygin and Sereno, 2008). Does this topological organization provide an understanding of the format of visual location representation? Vaguely. Many areas only have a coarse retinotopic mapping that shows contralateral correspondence between activations in neurons and information on the retina (Silver and Kastner, 2009). 

The theoretical gap is surprising also because we have gained a much detailed understanding of representations and computations for identity inferences, starting from edge detection (Marr & Hildreth,1980; Zhou, Friedman, & Von Der Heydt, 2000), to surface and shape perception (Nakayama, He, & Shimojo, 1995; Marr & Nishihara, 1978), to object recognition (Riesenhuber, & Poggio, 1999) and face identification (Kanwisher, McDermott, & Chun, 1997; Haxby, Hoffman, & Gobbini, 2000). The location information, however, seems to be absent, or implicit at best, in those algorithms. Thus, it remains unclear what representations of location are used in those algorithms. In short, neither does the widely-spread topological organization nor do the well-understood identity inference algorithms provide a rich, useful, and stable representation of space. Nor can they characterize the location representations within a space or clarify algorithms that process these representations.  

Before we develop further discussions and review empirical studies, here is an outline of this review.

The first section introduces computational problems of location. It starts with difficulties in location inferences in vision. For vision to overcome those difficulties, a good design of location representations needs to solve two problems. One is to specify a references frame that defines all locations involved in computations; the other is to choose a format that describes locations in the reference frame. A location representation also concerns computations that acquire them and operate on them. Thus, the first section ends with a review of a set of visual processes that involve location information.

Next two sections focuses on two problems in location representations.

The second section focuses on how to determine a reference frame. It reviews two categories of reference frames: an allocentric reference frame that characterizes distal locations in the environment that is independent of observers, and a egocentric reference that describes distal information in relation to the body of observers. To choose one over the other, a cognitive system has to account for advantages and disadvantages of each reference frame when the system attempts to solve a specific computational problem. The second section thus investigates visual reference frames via computational problems. 

The third section focuses on what is the format of location representation within a reference frame. Different formats of representation make some aspects of information explicit at the expense of making others implicit. Algorithms that process information can easily get access to the explicit part but may not be able to extract the implicit part; thus they may fail to solve a computational problem. A computational problem, perhaps, also requires different algorithms for distinctive representations of the same information. The third section surveys three formats that could represent location information: a propositional one, a nominal one, and the coordinate system. Our discussion finishes with comparisons among variants of coordinate systems, and paves the way to empirical investigation in the final thesis. 

## Challenges in visual location inferences 

Visual location inferences aim to accurately recover distal locations in the world. These inferences start with processing images on our retina. These retinal images, as mentioned above, are information-incomplete; they correspond to multiple possibilities in the distal world, and impose many challenges for visual location inferences.  

The first challenge is to recover depth and fill in contents in different distances from insufficient retina information.

When the three-dimensional physical world is projected on our two-dimensional retina, depth information is lost. This absent one-dimension of distance is still represented in our mind, because we still know the distance of objects out there. Such representation requires computations of available information on two retinae, information that provides cues such as perspective, occlusion, motion parallax, binocular disparity, and binocular convergence (Palmer, 1999). Without cues being available, people can be very bad at judging absolute distances of distal objects; they are inaccurate when the distance is more than 2 to 3 meters. With proper cues, people can accurately estimate depth within 20 meters (Wu, Ooi, & He, 2004). Obviously, depth is inferred by the mind, not directly extracted from the world.

The projection of the world to a space in lower dimension also results in occlusions or identity loss when multiple objects exist. The visual system needs to fill in what hides behind occlusions to understand an object that is far away and localize it in three-dimension space. For example, when a magician presents a pair of rabbit ears occluded by a tall hat, we cannot know whether this partially occluded rabbit is inside the hat, or actually behind the hat. Nor can we know for sure what is hidden, an intact rabbit or only a pair of rabbit ears. Our mind infers that there is a rabbit, and we seem to be content with that inference without further doubts. Our mind completes the missing pieces and draw inferences from what we could perceive. When such completion succeeds, we understand what we see; when such completion fails, we may be surprised or confused.  

Even when retinal images can capture complete information of distal objects, it is still a challenge for our visual system to process these images and to infer locations. For example, patients with neurological deficits, such as optic ataxia and William syndrome (Rossetti, Pisella, & Vighetto, 2003; Perenin, & Vighetto, 1988; Landau, & Hoffman, 2012), often fail to use retinal information to localize or relate their bodies to distal objects.

Normal observers can also misperceive locations in the same two-dimensional plane. Motion often induces misperception of locations. Observers fail to perceive actual turning points when a dot moves back and forth, and underestimates the moving distances of the dot (Sinico, Parovel, Casco, and Anstis, 2009). Observers also perceptually misalign a flashing dot with a moving dot when their retinal images of these two dots are actually aligned; they thus experience the flash lag illusion (Eagleman & Sejnowski, 2007). Even static images on their retina may be mislocalized when they are presented briefly. For example, when a dot flashes briefly after an anchor stimulus in peripheral visual field, observers localize the probe closer to the anchor than the probe’s true location. Observers may experience such illusory location compression when they keep their fixation (Born, Zimmermann, & Cavanagh, 2015; Zimmermann, Born, Fink, & Cavanagh, 2014) or when they are about to initiate a saccade from a fixation point to the anchor (Ross, Morrone, & Burr, 1997). In short, retinal images, even being faithful replicas of distal objects, are not sufficient for visual system to recover the true location in the world. 

A third challenge is to integrate rapid changes of retinal images and to compute correct correspondence of two images. 

Objects move in the world, and constantly change their locations in relation to us. We also move a lot. Even when our body stays static, our heads turn around frequently and our eyes jump restlessly. Between two of such jumps, the eyes only fixate steadily for about only 200ms to 300ms (Rayner, & Castelhano, 2007). The movements of distal objects, as well as our body, head, and eyes, change retina images rapidly. Thus, an object rarely has the same location on retina for a long time. In contrast to these rapid changes, we perceive a stable and static world. We can understand an object is static even though its projection on retina changes location. We can also discriminate whether an object is moving or we are moving even though two series of retinal images are the same. How can we infer an object is the same object when it changes its location on our retina? How can we even localize the object in the world when we have no consistent retinal images of it? Our visual system needs to apply a correspondence computation that identifies the same object across multiple images and uses these images to infer the location of this object. 

Despite all three challenging problems, our visual system makes reliable inferences of locations in the world. What does the visual system do? To probe its computations, we first try to understand how locations are represented in the visual system. 

## Questions in location representations

Our visual system represents locations either directly by extracting retinal information or indirectly by transforming and synthesizing retinal information. Such a representation, in order to describe a specific location, needs to establish a set of all possible locations and specify variables and rules in description (McCloskey, 2009). To former requires determining a reference frame, in relation to which a location is defined; the latter requires selecting a representational format, with which a description is characterized in detail.
 
For a choice of a reference frame, we can have a general spatial representation of an environment that is independent of our own body, or individual spatial representations that uses our body as reference. An allocentric reference frame uses landmarks in an environment to define locations, and provides an invariant environment, within which we may navigate or perceive an object invariant to our point of view. An egocentric reference frame, uses part of our body to define locations, and provides an invariant information-processing center for us, which coordinate afferent sensory signal and efferent motor signals. The choice of the reference frame may alter algorithms that solve the same cognitive problem. For example, when we navigate in a hall, our cognitive system may use an allocentric reference frame, such as a cognitive map of the hall, to guide us to our destination. Our cognitive may also use an egocentric reference frame, such as a frame centered at our fovea on the retina, to help us explore the hall and confirm our destination. With the allocentric reference frame, we can easily map out a route from our current location to the destination; with the egocentric reference frame, we have to constantly update our location from previous one and remap all the information between the update. In this case, the allocentric reference frame provides a straightforward solution with clear algorithmic steps; the egocentric requires a complex process with iterative and error-prone calculations. 

For a choice of representational format, it could use a set of propositions in a propositional system, such as an object A is TO THE LEFT of an object B, or an ordered tuple of numbers in a coordinate system, such as the object A is at (x, y) away from the origin. Other coordinate systems, such as a Polar coordinate system, or other completely different systems are also possible. Different formats, however, may provide accessible information for some computations but block information for others. For example, to calculate motion velocity and acceleration, it is easy to use a coordinate system but nearly impossible to use a propositional system. Thus we investigate the representation format by analyzing computations for visual location inferences.

## Role of location information in computations

Different representations of the same information may be isomorphic, and have one-to-one mappings between each other. Probing a representation per se may hardly help to discriminate one from another. But different cognitive computations, which usually depend on specific kinds of representations, may be distinctive. Thus we study computations to probe representations. 

Location representations are widely used in computations for visual perception. Depth, for example, changes the apparent size of perceived objects and result in multiple visual illusions, such as the moon illusion (Rock and Kaufman, 1962), and the Ames room illusion (Gregory, 2015).   

Location information usually interacts with identity information in visual computations, such as object recognition. Electrophysiology and neuropsychology research, however, has established two separate information pathway in the brain. A dorsal pathway, known as the “where” pathway or the “how” pathway, is responsible for location or visually guided action; a ventral pathway, known as the “what” pathway, is responsible for object recognition (Goodale & Milner,1992; Mishkin, Ungerleider, & Macko, 1983). A dogmatic disciple of such a two-pathway theory may fail to understand visual processing in both mind and brain, because any complex object is a composition of features in a particular spatial configuration.
 
First, spatial information helps to organize ambiguous local information and to build up an integrated object at the global scale. In analysis of local features of an object, feature representations are ambiguous. For instance, looking through an aperture, we misperceive the motion direction of a bar, because we only get access to insufficient local velocity information of the bar behind the aperture (Shimojo, Silverman, and Nakayama, 1989). But our global perception is stable and crisp and we usually process global objects better than its local features. Navon (1977) has shown that people can easily perceive a global pattern that is made of multiple small units. The figure below illustrates several examples. The capital letter H, for example, does not have a continuous and connected region. Instead, only multiple small Ss possess different locations in space. We nevertheless perceived the capital letter H without much effort. To perceive the H, we need to have correct locations of each component, the small Ss. If the perceived spatial layouts, the collections of the locations of each unit, are distorted, the global perception may change, such as becoming a capital letter A, or a Greek letter θ.  
 
![Figure 2](/assets/img/thesis-figure/figure2.png)  
Figure 2: Locations of the units determines the global perception of a configuration. On the left, we can see a letter H. If the perceived locations of elements of that H are distorted, we may perceive an A in the middle, or a θ on the right.

Building object-level perception from local signals is not only a cognitive science problem, but also an inevitable neuroscience problem. Most neurons in the visual systems only respond to stimuli in a limited region in the visual field. Such responsive regions in the visual field are called the receptive fields (Palmer, 1999). Neurons in the early visual system have small receptive fields, especially those that process information around the fovea. A neuron in the primary visual cortex of a monkey may only have a receptive field of 1 degree in size (Freeman & Simoncelli, 2011; Gattass, Gross, & Sandell, 1981). Human imaging experiments show a similar receptive field size in the human early cortex to those in monkey primary visual cortex. Recent developments of imaging and modeling techniques have shown that the population receptive field (pRF) for a voxel, the overall receptive field of a population of neurons, is about 1 degree in visual angle in the human primary visual cortex (Dumoulin, & Wandell, 2008). The small size of receptive fields means that each neuron can only see a small region of the distal world. But our final visual perception sees a large visual field. Connecting local information that is “seen” by each neuron to form a global perception requires accurate location information of each unit in the large configuration.

Second, spatial location affects our discriminations of complex objects. One special category is the face. In face recognition, locations of the elements, such as the eyes, the nose, and the mouth, are important for the correct understanding of the face. The locations of these elements consist statistical regularities, and together form “configurational information” of a face (Tanaka & Farah, 1993). When the locations of the elements are scrambled, we cannot perceive a face anymore, even though the basic elements are the same. When a face is just inverted upside down, where some of the relation locations of basic elements remained intact, our ability to recognize a face is also impaired (Yin, 1969). Sometimes we cannot even detect that a face is actually distorted when it is inverted, as revealed in the Thatcher effect (Thompson, 1980). 

![Figure 3](/assets/img/thesis-figure/figure3.png)  
Figure 3: the locations of the elements on face affect the recognition of the face.

More generally, for an object of multiple parts, the relative locations of its parts can be a key to differentiating it from other similar objects. For example, if a vertical line is attached to one end of a horizontal line, we perceive a letter L, whereas if a vertical line is attached to the middle of a horizontal line, we see a letter T. Chemists who study organic chemistry or biologists who investigate proteins can appreciate the importance of location information, because many organic compounds and proteins that only have subtle locations differences in their structures can have sharp differences in their properties and functions. Theories of shape and object recognition all address the location of the parts in relation to the main part of the object, such as the trunk (Marr & Nishihara, 1978; Feldman & Singh, 2006). So, it seems that in every "what" problem, there is a "where" problem.

In addition to the spatial layout that organizes features in an object, location information also bind multiple features, such as color, orientation, and texture, for each part of the object. The binding problem is treated as one of the most difficult problems in cognition. Its general form concerns the integration of multiple features or dimensions into one single entity (Roskies, 1999). Treisman (1996) categorized seven binding problems: 
1) property binding, which binds different properties to one single object that they characterize; 
2) part binding, which binds different parts of an object, possibly discontinuous due to occlusions, and separates the object from the background; 
3) range binding, which signals a particular value on a property by ratios of activity level in a population of neurons to its full range; 
4) hierarchical binding, which binds low-level visual properties, such as edge and texture, into mid-level visual property, such as a surface, or a high-level visual object; 
5) conditional binding, which interprets one property based on other dependent properties, such as motion and depth; 
6) temporal binding, which binds two objects across temporal interval; 
7) location binding, which binds objects to their current locations. 

We can see that spatial information prevails in these categories. In the temporal binding problem, for example, we need to have knowledge of where objects localize in space at a different times before we can bind the same objects across time. When such knowledge is ambiguous, the binding results are also ambiguous, as shown in the Ternus effects (Petersik, Rice, 2006). But such a detailed taxonomy may be more confusing than helpful and could over-complicate the problems. For example, if property binding is separated from location binding, properties seem to be “free floating” without attaching to any location. What would be the media or cues for these properties ever find their right “partners” to bind with? Besides, electrophysiology data suggests that properties may not “free float” without location specified, because neurons usually only respond to specific features at specific locations. Even if a neuron responds to features in a large region in the visual field, it is not sufficient to say that the properties are free floating. In contrast, if different properties have their own location indices, similar to those memory addresses of variables in the Random-Access Memory, these properties can be bound together with common location indices. In short, location information may participate in most computations that solve the binding problems.

Perceiving motion, especially long-range motion, also requires location information. In motion perception, a correspondence computation takes the locations of an object at a different time as inputs, and calculates its motion direction and its speed. When the location input is noisy, or the location perception is impaired, for example when the object is crowded by multiple nearby distractors, the motion perception is largely impaired: a moving object may be perceived as static (Ma, McCloskey, & Flombaum, 2015). 
 
Lastly, location information is involved in computations that provide interfaces between the visual system and other cognitive systems. For example, visual attention is usually directed to different locations, even if no objects have been presented there (Kastner, Pinsk, De Weerd, Desimone, & Ungerleider, 1999). Many models of visual attention, such as the saliency map model (Itti & Koch, 2000), the zoom lens model (Eriksen, & James, 1986), and many others, concern location information either as its input or output in the computation. Location representations can instruct visually guided action as well. Understanding the location of an object in space is necessary for reaching to this object. Brain lesions in the dorsal pathway that impair the computations of location information can lead to a failure in reaching the objects, whereas keep the object recognition ability intact (Mishkin, Ungerleider & Macko,1983). 
