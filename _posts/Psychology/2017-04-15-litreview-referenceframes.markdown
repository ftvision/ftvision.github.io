---
title: Determining the Reference Frames
subtitle: 
layout: single
---

A message can convey information when the message is a specific one from a set of all possible messages, as claimed in information theory (Shannon, 2001). A representation system that is used to carry location information needs to specify a set of all possible locations. To define this set, the representation system needs to determine a reference frame first. The reference frame defines all locations in relation to a group of invariants that are assumed static. 

If the reference frame is ambiguous or multiple reference frames are used interchangeably without care, a description of location can be confusing. A location in one reference frame may correspond to either a single location or a series of locations in another reference frame, and vice versa. For example, the Empire State Building occupies a specific location on Earth, and its location never changes as long as the location is defined in the reference frame of the Earth. The Empire State Building is stationary. However, if we can stand outside the Earth and define locations in the reference frame of the Galaxy, a specific location on Earth corresponds to multiple locations in the Galaxy, because the Earth rotates constantly. Accordingly, the Empire State Building rotates constantly in the reference frame of the Galaxy. Now, the Empire State Building is moving, and it has different locations at different time. Then, where is the Empire State Building? It is both stationary and moving. The location of the Empire State Building is uniquely defined only when we specify the reference frame in description. Thus, an unambiguous description of an object’s location requires a clear understanding of the reference frame. 

This is the **definition** question (McCloskey, 2009) when we investigate the location representations in a cognitive system: we need to understand what reference frame is used by this cognitive system when it represents a location. For an observer, be it a human, an animal, or a computer system, who constantly interacts with the environment, two sets of reference frames are possible: a set of egocentric reference frames that define locations in relation to the observer, and a set of allocentric reference frames that define locations in relation to a set of points or objects in the environment (Klatzky, 1998). To clarify which reference frame is used by our visual system to define locations, we search for the invariants that remain unchanged in visual processing. In the set of egocentric reference frames, different egocentric reference frames use different parts of the observer as the invariant factor. Such an invariant factor may be the origin point of a reference frame. For example, a human observer uses an eye-centered reference frame to process visual information (Engel, Glover, & Wandell, 1997), a head-centered reference frame to receive auditory information (Yantis, 2013), and a body-centered reference frame to plan and execute motor actions (Guenther, Bullock, Greve, & Grossberg, 1994). In the set of allocentric reference frames, invariant factors are reference points or objects used in the definition. For example, rodents as well as human use an environment-centered reference frame in navigation (O'Keefe, 1976; O'Keefe & Dostrovsky, 1971). This environment-centered reference frame defines a location in relation to multiple landmarks in the environment and forms a stable description of objects and locations in the context regardless of where an observer stands. People also use an object-centered reference frame to recognize objects (Marr, 1980; McCloskey, 2009). This object-centered reference frame defines a location in related to the target object, such as a face or a hammer, and provides a viewpoint-invariant representation of the object (Marr, 1980).

## Egocentric Reference Frames

People interact with the world by receiving sensory information and exerting actions. Afferent sensory signals, such as visual, auditory, and tactile signals, converge to the brain via different pathways. Our cognitive system needs to integrate these signals in order to have a consistent conscious experience, and sends motor signals to our muscle and coordinate our body. Computations for these integration and coordination processes can be convenient by choosing a set of egocentric reference frames, with only minor differences in their origins at different parts of the body. These egocentric reference frames all define locations in relation to our body, thus can provide a similar information structure for each modality and help the cognitive system align, correspond, and combine different information. Locations information in these similar reference frames can also transform from one to another with less computational errors. Evidences have shown that our sensory and motor system indeed usually adopts an egocentric reference frame.

Vision is the most important component in our sensation and perception. Psychophysical and physiological studies have suggested that many visual computations use an eye-centered reference frame.

Most visual afterimages or aftereffects stay at a location in the eye-centered reference frame. If you fixate at a bright red dot for several seconds and then look at a white wall, you can see a green dot, which is the color afterimage of the previous red dot, on that white wall. This green dot always locates at your fixation no matter where you look. The same retinal location-specific afterimage also applies to a dot that is presented in the peripheral visual field to begin with. In addition, a prolonged view of specific visual stimuli can affect the perception of other subsequent visual stimuli. The previous stimuli only exert such visual aftereffects on subsequent stimuli that posses the same eye-centered locations. For example, in the tilt aftereffect, adaptation to a tilt adaptor line results in a biased perception of a vertical test line: the test line looks tilted away from the adaptor’s orientation. But such biased perception only happens in the same retinotopic location (Mathôt & Theeuwes, 2013). A test line that preserves the location on the screen but changes location on the retina does not suffer from the bias due to the adaptor (Mathôt & Theeuwes, 2013; Knapen, Rolfs, Wexler, & Cavanagh, 2009). Similarly, a prolonged view of motion direction only affects the direction perception of subsequent motion stimuli in the same retinotopic location (Wenderoth, & Wiese, M. 2008; Knapen, Rolfs, Wexler, & Cavanagh, 2009; c.f. Turi & Burr, 2012). The same retinotopic location specificity also applies to the face perception that a prolonged view of a face affects the gender and identity judgment of a test face (Afraz & Cavanagh, 2008, 2009). 

In addition to visual aftereffects, visual attention also seems to operate on retinotopic locations, in the eye-centered reference frame. Golomb and her colleagues combined the Posner-cue task (Posner, 1980) and a two-step-saccade task to investigate which reference frame is used in visual attention. When a stimulus was presented on the screen and an observer looks at it, this stimulus has two locations: it has a screen-based location in the reference frame of the screen and a retinotopic location in the reference frame of the retina. If the observer makes a saccade, her fovea corresponds to two different screen locations before and after the saccade, and we can study how information processing differs in the same screen-based location from the same retinotopic location. Using this experimental design, Golomb and her colleagues have shown that if a location is pre-cued before the saccade, visual discrimination performance after the saccade is better at the same retinotopic location than the same screen-based location (Golomb, Chun, & Mazer, 2008; Golomb, Nguyen-Phuc, Mazer, McCarthy, & Chun, 2010). This retinotopic performance advantage, however, only preserves for 100 ms after the saccade before it decays (Golomb, Pulido, Albrecht, Chun, & Mazer, 2010). Later, another performance advantage independently builds up at the screen-based location (Golomb, Marino, Chun, & Mazer, 2011). 

Visual neuroscience research also shows that different brain areas process visual information in an eye-centered reference frames.

Visual processing in the brain starts with the images on the retinae. From the retina to different levels of hierarchy in the visual cortices, there is a one-to-one correspondence relationship between locations in visual cortices and locations on the retina. That is, information in neighboring locations on the retina is processed by neurons in neighboring locations in visual cortices. This topological mapping relationship is called the retinotopic map of the visual cortex (Engel, Glover, & Wandell, 1997). Patient studies (Holmes, 1944), electrophysiological studies (Hubel & Wiesel, 1977), and neuroimaging studies (Engel, Glover, & Wandell, 1997) have shown a clear retinotopic map in early visual cortices, including the primary visual cortex (V1) and its neighboring areas. These retinotopic maps suggest that low-level visual information is represented in the eye-centered reference frame. Recent neuroimaging studies have started to show that this eye-centered reference frame is used beyond early visual cortices. In the dorsal pathway, multiple distinct sub-regions in intra-parietal sulcus (IPS) show retinotopic maps in response to rotational and expanding checkerboard stimuli (Bettencourt & Xu, 2016) or memory-guided saccade tasks (Schluppeck, Glimcher, & Heeger, 2005; Schluppeck, Curtis, Glimcher, & Heeger, 2006). Even further, the Frontal Eye Field (FEF) in the frontal region also shows a retinotopic map in saccade-related tasks (Kastner, DeSimone, Konen, Szczepanski, Weiner, & Schneider, 2007). These retinotopic regions that share the same topological organization of visual information are suggested to provide spatial information in the eye-centered reference frame for computations in visual attention and eye movement (Silver & Kastner, 2009). 

In the ventral pathway, it remains unclear whether a retinotopic map exists. Recent neuroimaging studies, mostly using multi-voxel pattern analyses, also found that neighboring neurons contain information of neighboring areas in the visual field. When eyes shift, neurons respond to same perceptual locations on the visual field, not to the same screen-based locations on the monitor (Fischer, Spotswood, & Whitney, 2011; Golomb & Kanwisher, 2011).  

Additionally, retinotopic maps are found in subcortical structures, such as superior colliculus (Katyal, Zughni, Greene, & Ress, 2010; Schneider & Kastner, 2005) and pulvinar (Bender, 1981; Fischer & Whitney, 2012), which integrate multi-sensory information and participate in attention competitions. 

Beyond visual perception, auditory and multi-sensory perception is suggested to use a head-centered reference frame. Auditory system uses the inter-aural differences, such as arriving time and amplitude, in sound information to calculate the location of the sound source. The inter-aural information changes as the head orientation changes, and the calculated location is represented in the head-centered reference frame (Yantis, 2013). Moreover, when auditory information and visual information are integrated to infer a distant object, both eye-centered and head-centered reference frames are used (O'dhaniel, Cohen, Groh, 2005). 

When multiple sensory and motor systems collaborate and coordinate the location information, the centers of the egocentric reference frames shift to different parts of the body. For example, when visual cue guides the facial movement, a head-centered reference frame is used in the parietal face area (Sereno & Huang, 2006). When visual information guides arm movements, information in the head-centered reference frame was transformed into a shoulder-centered reference frame (Soechting, Tillery, & Flanders, 1990). In reaching and other sensorimotor control tasks, information in both eye-centered and head-centered reference frame are combined and mapped onto a body-centered reference frame to plan, coordinate, and execute the movements of different body parts (Guenther, Bullock, Greve, & Grossberg, 1994). 

Although egocentric reference frames are useful and widely exists in our cognitive system, they are not convenient to solve all computational problems. Because they strongly depend on the observer, the egocentric reference frames can only represent local information that is experienced by the observer, and is constrained by the scope within which the observer interacts with the environment. These reference frames may lack global, objective descriptions of an environment or an object with which an observer can extrapolate information beyond her experiences, such as a shortcut in the building between her office and the coffee shop. These egocentric reference frames may also require cumbersome computations to solve cognitive tasks such as navigation and viewpoint invariant object recognition. In these cases, an allocentric reference frame, which represents information independent of the observer, would be useful. 

## Allocentric Reference Frames

Let’s start with an example of navigation to understand the computational advantages of allocentric references. You walk around the Ames Hall and you want to know where your office is and how far away it is from you. How will you mind describe location information to compute the direction and distance to your office? Of course, your mind can use an egocentric reference frame that provides location information. But any algorithms that use such information need to constantly update the location of the office when you walk around and recalculate the distance. These algorithms can accumulate errors in each update or fail when proper updates were interrupted, such as when you are dis-oriented. In contrast, your cognitive system may represent all the locations in the reference frame of the Ames Hall, forming a cognitive map (Tolman, 1948), and label your office and your own location in this map. Once this representation is established, no matter where you go, your cognitive system now can retrieve the location of your office simply by reading out the information without further transformations. You cognitive system can also calculate the distance between you and your office directly from the coordinates of these two locations. By defining locations independent from the body of observers, allocentric reference frames can reduce expensive, iterative location transformations to a one-time retrieval of the observer’s location.

Navigation and memory studies of the hippocampus system show that it represents the space in an environment-centered reference frame (O'Keefe, 1976; O'Keefe & Dostrovsky, 1971). Animals gradually build a topological map of the environment and can also update such a map when they enter new environments (Lever, Wills, Cacucci, Burgess, & O'Keefe, 2002). Learning the environment seems to start with an egocentric representation and then develop into an allocentric representation. In the cross-armed maze learning task, a rat always starts with the same location and searches for the food reward. The rat can achieve the place-learning stage when it knows the location of the reward in the maze regardless of its own location. Such learning shows that the rat represents the space in an allocentric reference frame, and it could successfully find the reward even when it starts at a new location in the maze (Packard & McGaugh, 1996). A chemical lesion in hippocampus system prevents the rat from place learning, suggesting that the hippocampus system uses the allocentric reference frame of the environment in navigation (Packard & McGaugh, 1996).

An allocentric reference frame centered at an object also has computational advantages – it provides a convenient representation to achieve invariant object recognition from different viewpoint, under different lighting conditions, or in different shades and scales (Marr 1980). Neuropsychological and psychophysical studies have shown evidence for object-centered representations. For example, McCloskey and his colleagues (2009, 2010) have found that people frequently make mistakes between an image and its mirror reflections. Particularly, in a neurological case, a student called AH constantly misperceive an image to be its reflection forms in reading, drawing, pointing, and other tasks (McCloskey, 2009). Normal observers also make more reflection mistakes than other orientation mistakes (Gregory& McCloskey, 2010). Because the reflection of an image is in relation to the image’s own axis and reflection mistakes exist regardless of the image location in relation to the observer, errors in performances suggests visual recognition uses an object-centered representation that is independent of the observer. Similarly, some visual neglect patients only ignore half of every present object in the whole visual field. They can draw all the objects in a target picture, but they only draw one side of each object, as if the other side does not exist  (Driver & Halligan, 1991; Halligan, Fink, Marshall, & Vallar, 2003). These results suggest that visual attention can also operates in object-centered representations. Behavioral studies revealed that such object-based attention also exists in normal people. For example, people respond faster to a location in a novel part of the cued object than a location with equal distance but in a novel object (Egly, Driver, & Rafal, 1994). 

Similar to the egocentric reference frames, allocentric reference frames have their own disadvantages and may not suit for several computations. Allocentric reference frames cannot use information directly from the sensory system. Information in these observer-independent reference frames needs to be built up gradually from multiple pieces of direct sensory information from those egocentric reference frames (Klatzky, 1998). They also cannot directly send signals to motor system, because they do not represent the body of observers. Therefore, information needs to be transformed between allocentric and egocentric references frames when two systems with different choices want to interact with each other. 

## Interaction between egocentric and allocentric reference frames in vision

Egocentric reference frames and allocentric reference frames have their own merits and drawbacks when a cognitive system uses them to define locations. Here we discuss the interaction between them in two computational problems in visual cognition. 

The first problem asks how can we have a stable sense of the world given our restless saccades. One possibility is that our visual system uses an allocentric representation of the world that is stable and independent of where our eyes look at (Breitmeyer, Kropfl, & Julesz, B, 1982). The other possibility, which has gained more supporting evidence in recent research, is that our visual system can update mapping functions between information in visual areas and that on retina before saccades such that old information on retina can be remapped onto new information at new retinal location according to the saccade’s direction (Cavanagh, Hunt, Afraz, & Rolfs, 2010). Neurons in frontal and parietal cortices have found to shift their receptive fields before a saccade is executed, and arrive at the predictive location in the visual field (Wurtz, 2008). Though the details of remapping mechanisms are still in debate (Cavanagh, Hunt, Afraz, & Rolfs, 2010), this function seems to provide an algorithmic solution for an egocentric location representation to achieve the stable sense of the world.

Although the remapping function suggests a possible algorithm that integrates information in an egocentric reference frame into a stable conscious perception, it is not sufficient or necessary to exclude that visual system does not use an allocentric reference frame. First, such remapping may introduce and accumulate mapping errors when multiple saccades follow each other. Second, such remapping may overshadow an underlying allocentric representation that is closed related to spatial representation in hippocampus. After all, our stable sense of the world is not only in vision; our cognitive integrates all sensory information and presents us a stable multi-sensory experience of the world.
 
It would be useful to think about the Google street view system as an example, and consider how such an artificial system generates a stable representation of the world. Such a system could be decomposed into two components: a front-end component that is responsible for rendering images that present you the street view, and a back-end component that is responsible for computations of locations and for guiding front-end to render proper visualizations. The front-end component, to visualize a coherent world image, needs to constantly match different image segments, remap objects in one segment onto those in another, and finally stitch these segments into a panorama. When you click a new location, the front-end has to render a new panorama again by remapping objects in previous panorama onto the new one. Indeed, such front-end component can coordinate different image segments and visualize a seemingly stable world on the screen. It is the back-end component, however, that tells what segments should be stitched together when the user clicks at a location on the map. This back-end component uses a probabilistic graphical model, a specific computational model with structures of probability distributions, to represent the road network system in the world (Anguelov, Dulong, Filip, Frueh, Lafon, Lyon, Ogale, Vincent, & Weaver, 2010). Such world model organizes locations of buildings and streets independent of what is visualized on the screen. In fact, it is the back-end that stores allocentric information for front-end to retrieve new image segments and add these new segments to its next visualization step.

Similarly, despite that it can remap retinal information between saccades, our visual system may still need to get reliable location information from an allocentric reference frame, because the allocentric reference frame can provide more precise descriptions of a location in the world regardless of where the body, the head, or the eye is. Where does this allocentric reference frame exist in the visual system? The answer still remains unknown. Maybe we should first ask whether visual processing is really responsible for the stable sense of the world, or it only serves to match and stitch multiple retinal images so that another independent system can use to construct a stable world model, which may be the real reason for the stable sense of the world.

The second problem asks how can we know a location of a moving object. More often than not, we misperceive a location in motion and experience motion induced position illusion, such as the flash drag illusion (Eagleman & Sejnowski, 2007) and the flash grab illusion (Cavanagh & Anstis, 2013). Our illusory location perception shows that locations of distal stimuli cannot be inferred simply from the retina image or solely in an eye-centered reference frame. I conjecture that locations of distal stimuli are separately computed in an allocentric reference frame module and an egocentric reference frame module. When inferring the location, our visual system integrates location information from two modules with the help of knowledge in other domains, such as knowledge of objects or physical laws. 

For example, people can recognize a human motion pattern that is presented by only several dots (Johansson, 1973). Moreover, people can recognize the gender (Mather & Murdoch, 1994) and motion state (Johansson, 1973) of this human-of-dots. This biological motion perception is possible only when the observer interprets the dots in a human-centered reference frame first: the arms and legs move relative to the body. Then, the locations of the dots are computed in the eye-centered reference frame by taking the body motion into account. Attributing dots to a human-centered reference frame is even more important when people perceive two sets of dots as two people are dancing with each other (Su, 2016).

Allocentric reference frames also interact with the eye-centered reference frame when we infer a location in moving. For example, when an object is on a moving background, we cannot simply isolate this object and only use its corresponding retinal image to infer its location in the world. The flash grab illusion (Cavanagh & Anstis, 2013), for example, have shown that we mislocalize the object by the motion of the background. When a vertical line briefly presented at the moment when the background rotating disc turning its motion direction, we perceive the bar to be tilted towards the new motion direction. Although no mechanism explanation is provided to this illusion (Cavanagh & Anstis, 2013), it is possible that such illusion is resulted from a reasonable interpretation by the physical law. The physical law predicts that an object that stays stationary on another moving background should move along with the background. Recall the example of the Empire State Building, it moves along with the Earth when the Earth rotates around its own axis. In our visual perception, the briefly presented vertical line is perceived to be in the rotating disc background, thus we believe it should rotate along with the disc, no matter how brief its presentation is, according to the physical inference. This hypothesis predicts that if we can use optical manipulations to separate the vertical line from the surface of the rotating disc, such as rendering them in two different depth planes, our flash grab illusion may decrease or disappear. No matter whether this hypothesis is correct or not, above-mentioned examples suggest that we can analyze location information in different reference frames and synthesize these analysis to have a better understanding of visual phenomena and computations. 
