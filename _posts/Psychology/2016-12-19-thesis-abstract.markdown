---
title: Abstrat
subtitle: 
layout: single
---

There are two basic computational problems in vision: What and Where. The former concerns object recognition; the latter concerns spatial localization. Spatial location is not only important in and of itself but is also essential for other visual computations. To name a few, motion perception calculates the direction and speed based on differences between locations at successive time points. Visual attention operates on spatial locations, selecting one place at a time. Visually guided action is derived from the understanding of the spatial location of objects and their parts, based on which appropriate motor responses are planned and executed. Lastly, object recognition itself also requires an understanding of the spatial configuration of different parts. For example, a scrambled face cannot be and should not be recognized as a face. 

A spatial location becomes meaningful only when a reference frame is specified. For example, a location of (3,3) in the world is an undefined location. Only when a reference frame, such as a computer screen, is determined, and the origin and a coordinate system is specified, the location of (3,3) becomes well defined and refers to a specific location on the computer screen. As illustrated by this example, we need to understand in what reference frame a spatial location is represented. To understand a reference frame, we need to answer two important questions: **The first question is the reference point question: in relation to what are locations defined. The second question is the format question: In what form are locations defined.**
 
Most previous research focused on addressing the first question. For example, in a general setting of visual experiments, a visual stimulus is presented on a computer screen in front of an observer. The spatial location of this stimulus can be discussed in at least the following four reference frames. First, it has a location relative to a reference point, such as the center, on the computer screen. Thus, it has a location in the screen-center frame of reference. Second, the stimulus is projected on the observer’s eyes. It has a location on the observer’s retinae relative to the observer’s fovea. Thus, it has a location in the eye-centered frame of reference. Similarly, the stimulus has a location in the head-centered frame of reference defined by the observer’s head orientation, and a location in the body-centered frame of reference defined by the observer’s body position. Previous studies have investigated the reference point question in these different frames of reference and showed that different cognitive systems represent and calculate locations relative to different reference points.

It is only the first step to understanding the computations of spatial location by studying the reference point of a reference frame. The computation of locations and relationships between locations depends on the format of the reference frame. A naïve format of a reference frame could be a lookup table of multiple units, with the first unit as the reference point. Each unit represents a location in the reference frame as a nominal variable. In such a reference frame, we can only know whether a location exists or not. No relative relationship, such as the direction and the distance between two locations, can be easily computed. Nor can it represent the infinite number of locations in 1D, 2D, or 3D space. Alternatively, a reference frame could be a coordinate system that uses an ordered tuple of one or more real numbers to specify a location in the reference frame in relation to the reference point. For example, a 2D Cartesian coordinate system uses a pair of numbers, (x, y), to indicate a location in a 2D plane, which has the signed distances to the reference point from two orthogonal directed lines. Another commonly used 2D coordinate system is the Polar coordinate system, which indicates a location by determining its distance from a reference point and its angle from a reference direction, denoted as a pair of real numbers, (r, Θ). These two coordinate systems can easily afford computations on locations, such as distance, direction, or orientation. 

The current dissertation work focuses on investigating the format of the reference frame that is used in visuospatial representation. It is implausible that a look-up table format is used in the visual system given the contrast between computational constraints of a look-up table and vast computations that have been shown in visual processing. This research thus focuses on identifying which coordinate system is used in the visual system. 

The information transformation from the retina to the primary visual cortex suggests that a polar coordinate system is used in visuospatial representation. The retinotopic mapping in early visual cortex follows a logarithmic transformation of polar coordinates on the retina. In the striate cortex, the posterior-anterior axis represents the distance to the fovea, and the dorsal-ventral axis represents the polar angle from the lower visual field to the upper visual field. The primary visual cortex is the first stage of cortical visual processing and its representation of spatial location is used as the input for the subsequent computations. Functional anatomy research of visual cortex further shows retinotopic maps in the extrastriate cortices. It is possible that the whole visual system uses the polar coordinate system in spatial location computations. 

One problem of the retinotopic polar coordinate system is that the represented locations depend heavily on the reference point, that is, the fovea. Thus a location representation in such a coordinate system is constrained by what the eyes look at. Vision, ultimately, needs to infer the location of objects in the world regardless of where the observer stands or looks, and to form a viewer-independent representation of the spatial location. The Cartesian coordinate system can provide easier computations on spatial locations in such a case. For example, an affine transformation, such as translation, rotation, and scaling of locations, can be easily processed by matrix operations in the Cartesian coordinate system. In addition, some motor system, such as the saccade system, functions in a Cartesian coordinate system with a horizontal channel and a vertical channel. To accommodate computations for viewer-independent location and visually guided action, the Cartesian coordinate system seems to be an intuitive choice. 

It is difficult to dissociate the polar coordinate system from the Cartesian coordinate system because these two systems have a one-to-one correspondence from one system to another. Investigating a single location in a reference frame cannot tease them apart. The current approach is to study computational problems that involve multiple locations, such as dot alignment, global pattern perception, or distribution of spatial uncertainty, and investigate the differences in algorithms used by different coordinate systems to solve the same computational problem. For example, to describe a circle, the Cartesian coordinate system needs to calculate a quadratic function of horizontal and vertical channels, whereas the Polar coordinate system only needs to calculate a linear function of distance and polar angle. By constructing different tasks that involve computations of multiple locations, I intend to compare human behaviors with alternative mathematical models of different coordinate systems to understand which model is more plausible in understanding the human behaviors.  
